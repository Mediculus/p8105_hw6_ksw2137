---
title: "P8105 Data Science I - Homework 6"
author: "Kevin S.W.   UNI: ksw2137"
date: "`r format(Sys.time(), '%x')`"
output: github_document
---

```{r setup, include = FALSE, message = FALSE, warning = FALSE}

# global default settings for chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width = 10, 
                      fig.align = "center"
                      )

# loaded packages; placed here to be able to load global settings
Packages <- c("tidyverse", "dplyr", "modelr", "mgcv", "patchwork", "corrplot", "scales")
invisible(lapply(Packages, library, character.only = TRUE))

# global settings for color palettes
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

# theme global setting for ggplot
theme_set(theme_minimal() + 
            theme(legend.position = "bottom") +
            theme(plot.title = element_text(hjust = 0.5, size = 12),
                  plot.subtitle = element_text(hjust = 0.5, size = 8),
                  plot.caption = element_text(hjust = 0))
          )

```

# Problem 1 - Birth Weight Modelling
First load our birthweight data.

```{r birthweight}

# though this part doesn't require seed, will set here because further problem requires it.
set.seed(2)

# reading in source data
birthweight_df <- read_csv("./data/Birthweight.csv") %>% 
  janitor::clean_names() %>% 
  mutate_at(.vars = vars("babysex", "malform", # convert columns in vars() to factors
                         "mrace", "frace"
                         ),   
            .funs = funs(factor)) %>% 
  mutate(
    babysex = fct_recode(babysex, male = "1", female = "2"),
    malform = fct_recode(malform, absent = "0", present = "1")
    )

#skimr::skim(birthweight_df)                   # remove "#" to activate skimr check 

```

After loading our birthweight data, we obtain a `r nrow(birthweight_df)` by `r ncol(birthweight_df)` dataset that all started as numeric variables. `babysex`, `malform`, `frace`, `mrace` were converted to categorical variables as they should.

Variables are:

* Baby-related:
  * `bwt`: baby’s birth weight (g)
  * `blength`: baby’s length at birth (cm)
  * `bhead`: baby’s head circumference at birth (cm)
  * `babysex`: baby’s sex (male = 1, female = 2)
  * `gaweeks`: gestational age in weeks
  * `malform`: presence of malformations that could affect weight (0 = absent, 1 = present)
* Mom-related:
  * `delwt`: mother’s weight at delivery (lbs)
  * `wtgain`: mother’s weight gain during pregnancy (pounds)
  * `ppwt`: mother’s pre-pregnancy weight (pounds)
  * `ppbmi`: mother’s pre-pregnancy BMI
  * `mheigth`: mother’s height (in.)
  * `momage`: mother’s age at delivery (years)
  * `menarche`: mother’s age at menarche (first menstruation; in years)
  * `mrace`: mother’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other)
  * `parity`: number of live births prior to this pregnancy
  * `pnumlbw`: previous number of low birth weight babies
  * `pnumsga`: number of prior small-for-gestational-age babies
  * `smoken`: average number of cigarettes smoked per day during pregnancy
* Family/father-related:
  * `frace`: father’s race (1 = White, 2 = Black, 3 = Asian, 4 = Puerto Rican, 8 = Other, 9 = Unknown)
  * `fincome`: family monthly income (in hundreds, rounded)

`skimr::skim()` reveals no missing variables. However, it reveals some variables that likely shouldn't even be considered; `pnumlbw` and `pnumsga`, though measures numbers of babies, only records `0` in this dataset.

### Model Proposal
We will attempt to make a model with `bwt` as our outcome of interest. Given that there are a lot of variables related to each other, there are likely a lot of correlations. We can then use a correlation matrix to see which variables are highly correlated and we can pick one out of those groups.

```{r cor_plot}

# correlation matrix
select_if(birthweight_df, is.numeric) %>%     # select numeric variables
  cor() %>%                                   # get the correlation values
  corrplot()                                  # make the correlation matrix

```

Based on the plot above, we see that variables most correlated to `bwt` is `bhead`, `blength`, `delwt`, `gaweeks`, `wtgain`, `mheight`, and `ppwt`. However, we also note that `delwt` is highly correlated with `ppwt`. Given that `delwt` is more correlated to `bwt`, we'll keep `delwt` and drop `ppwt`. It is also slightly correlated to `wtgain`, so we'll drop `wtgain` (lower collinearity to `bwt`) to prevent collinearity. Finally, we would think that `mrace` (mother's race/ethnicity) might also influence a baby's weight (genetically related). So our proposed model:

* Proposed model: `bhead`, `blength`, `delwt`, `gaweeks`, and `mrace`; main effects only.

```{r model_build}

# our first model try
first_fit_lm <- birthweight_df %>% 
  lm(bwt ~ bhead + blength + delwt + gaweeks + mrace, data = .)

# AIC, R-squared, etc
first_fit_lm %>% 
  broom::glance() %>% 
  knitr::kable()

```

Using `broom::glance()` shows that our adjusted R-squared is at a pretty good value of `r round(first_fit_lm %>% broom::glance() %>% select(adj.r.squared), 3)`, which means `r round(first_fit_lm %>% broom::glance() %>% select(adj.r.squared), 3) * 100`% of the variability in `bwt` is explained by our covariates. 

```{r summary_stats}

# statistics of each coefficient
# as well as replacing the label to be more explicit
first_fit_lm %>% 
  broom::tidy() %>% 
  mutate(
    term = str_replace(term, "^mrace", "Mom's Race: "),
    term = str_replace(term, c("1", "2", "3", "4"), c("white", "black", "asian", "puerto rican")),
    p.value = scales::scientific(p.value, digits = 3)
    ) %>% 
  knitr::kable()

```

We see from our coefficient summary that each variable have a significant p < 0.05 except for Asian mothers (White as the reference race). appears to be a reasonable compromise. 

### Checking assumptions

We then check our normality assumption of our model by plotting residuals vs. fitted values.

```{r resid_fitted_plot}

birthweight_df %>% 
  add_residuals(first_fit_lm) %>%                   # adding residuals to our df
  add_predictions(first_fit_lm) %>%                 # add predictions using our df
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.2, color = "steelblue1") +
  geom_smooth(method = "lm", color = "red") +
  labs(x = "Fitted Birth Weight (grams)",
       y = "Residuals",
       caption = "Figure 2: Scatterplot of residuals vs. fitted prediction values")

```

This is great since we see that our scatter plot seems to be centered at 0 without much variability. This means that our assumption of normality holds.

### Model validation
We can then compare our model to the other two proposed models: 

* Model 1: length at birth, gestational age as predictors (main effects only)

```{r model1}

# comparison model 1
model1 <- birthweight_df %>% 
  lm(bwt ~ blength + gaweeks, data = .)

```

* Model 2: head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r model2}

# comparison model 2
model2 <- birthweight_df %>% 
  lm(bwt ~ bhead * blength * babysex, data = .)

```

We will use cross validation in order to compare our models with these two by looking for the lowest RMSE as we are trying to evaluate the effects of various variables on birth weight.

```{r cross_validate}

# splitting our df into the default 80:20 
cv_df <- crossv_mc(birthweight_df, 1000) %>% 
  mutate(                                     # converts the resulting lists into tibbles
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )


# split process to relief some processing power
# applies model estimation to each dataset with the 3 models
trained_cv_df <- cv_df %>% 
  mutate(
    prop_lm = map(train, ~lm(bwt ~ bhead + blength + delwt + gaweeks + mrace, data = .x)),
    mod1_lm = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod2_lm = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x)))
    
# obtain rmse for each dataset in each model-set
trained_cv_df <- trained_cv_df %>% 
  mutate(
    rmse_Proposed = map2_dbl(prop_lm, test, ~rmse(model = .x, data = .y)),
    rmse_Model1 = map2_dbl(mod1_lm, test, ~rmse(model = .x, data = .y)),
    rmse_Model2 = map2_dbl(mod2_lm, test, ~rmse(model = .x, data = .y))
    )

# plots distribution of rmse by each model
trained_cv_df %>% 
  select(starts_with("rmse")) %>%              # select the rmse columns
  pivot_longer(everything(),
               names_to = "model", 
               values_to = "rmse",
               names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>%       # arrange by model
  ggplot(aes(x = model, y = rmse, color = model, fill = model)) + 
  geom_violin(alpha = 0.3) +
  labs(x = "Model",
       y = "Root Mean Squared Error",
       caption = "Figure 3: Comparison of the RMSE distribution between models") +
  theme(legend.position = "none")

```

Per our plot above, it appears that our proposed model have the lowest RMSE. Given this result, I would recommend the model that contains `bhead`, `blength`, `delwt`, `gaweeks`, and `mrace` as main effects only.

# Problem 2 - Weather Data
In this problem we'll be using weather data for Central Park, NY in 2017. Code adopted from homework website. 

```{r CP_2017_load, cache = TRUE}

# loads weather data; code from homework website.
weather_df <- 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

```

After loading the data, we should clean it up a bit and only select the things we need before running a bootstrap and obtaining the r̂-squared for each resampling as well as log(beta0*beta1). 

```{r df_clean}

# selects relevant columns and renames Central Park to cp_ny for simplicity
strapped_weather_df <- weather_df %>% 
  select(name, tmax, tmin) %>% 
  mutate(
    name = str_replace(name, "CentralPark_NY", "cp_ny")
  ) %>% 
  bootstrap(n = 5000) %>%                                 # runs bootstrap x5000
  mutate(
    model = map(strap, ~lm(tmax ~ tmin, data = .x)),      # obtains model for each strap sample
    result = map(model, broom::tidy),                     # obtains estimate data from each model
    stat = map(model, broom::glance)                      # obtains statistics from each model
  )

# a cleaned and filtered df containing variables of interest
clean_weather_strap <- strapped_weather_df %>% 
  select(-model, -strap) %>%                              # removed the original strap sample and model
  rename("strap_run" = .id) %>%                           # renamed id to "run numbers"
  unnest() %>%                                            # unnest the tibbles that remained
  select(strap_run, term, estimate, adj.r.squared) %>%    # selected relevant measures
  mutate(
    term = case_when(term == "(Intercept)" ~ "beta0",     # renamed intercept to beta0
                     term == "tmin" ~ "beta1",            # renamed slope to beta1
                     TRUE ~ as.character(term))
  ) %>% 
  pivot_wider(names_from = term,                          # put the betas into different columns
              values_from = estimate) %>% 
  mutate(
    estimate_log = log(beta0 * beta1)                     # add column for the ln(b0 * b1)
  ) %>% 
  janitor::clean_names()

```

After cleaning and bootstrapping, we isolated only the variables we're interested in, which is stored in `clean_weather_strap` that contains `r nrow(clean_weather_strap)` observations for `adj_r_squared`, `beta0`, `beta1`, and `estimate_log` labeled by `strap_run`. We can check their distribution using a simple density plot below.

```{r density_plots}

# density plot of r-squared with 2.5% and 97.5% quantile labeled
r_squared_plot <- clean_weather_strap %>% 
  ggplot(aes(x = adj_r_squared)) +
  geom_density(color = "seagreen2", fill = "seagreen2", alpha = 0.3) +
  geom_vline(xintercept = c(quantile(pull(clean_weather_strap, adj_r_squared), probs = 0.025),
                            quantile(pull(clean_weather_strap, adj_r_squared), probs = 0.975)),
             linetype = "dashed",
             color = "red") +
  labs(x = "Adjusted R-squared",
       y = "Frequency",
       caption = "Distribution of adjusted R-squared (left) and ln(β̂1 * β̂) (right)")

# density plot of ln(beta0 * beta1) with 2.5% and 97.5% quantile labeled
estimate_log_plot <- clean_weather_strap %>% 
  ggplot(aes(x = estimate_log)) +
  geom_density(color = "steelblue1", fill = "steelblue1", alpha = 0.3) +
  geom_vline(xintercept = c(quantile(pull(clean_weather_strap, estimate_log), probs = 0.025),
                            quantile(pull(clean_weather_strap, estimate_log), probs = 0.975)),
             linetype = "dashed",
             color = "red") +
  labs(x = "ln(β̂1 * β̂0)",
       y = "Frequency")

# use patchwork to print them side-by-side
r_squared_plot + estimate_log_plot

```

Plot suggests that both the r-squared and log(beta0 * beta1) are normally distributed. However, note that there is a very slight left skew on the distribution of r-squared, which shouldn't greatly deviate from our assumptions of normality. The red, dashed line represents our 2.5% and 97.5% quantile which are obtained using the `quantile()` function below.

```{r quantiles}

# r-sq 95% CI
quantile(pull(clean_weather_strap, adj_r_squared), probs = c(0.025, 0.975))

# est_log 95% CI
quantile(pull(clean_weather_strap, estimate_log), probs = c(0.025, 0.975))

```

As such, we have a 95% CI of [0.893, 0.927] for adjusted R-squared and [1.966, 2.058] for ln(β̂1 * β̂0).